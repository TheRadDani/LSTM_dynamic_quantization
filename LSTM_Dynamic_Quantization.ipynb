{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/TheRadDani/LSTM_dynamic_quantization/blob/main/LSTM_Dynamic_Quantization.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0Su5-WXsCZ3J"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "from io import open\n",
        "import time\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "! git clone https://github.com/pytorch/examples.git"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "q3HbmMh4KTOT",
        "outputId": "ea863d3e-2c55-4c9d-b344-40864f8fb426"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'examples'...\n",
            "remote: Enumerating objects: 4348, done.\u001b[K\n",
            "remote: Counting objects: 100% (65/65), done.\u001b[K\n",
            "remote: Compressing objects: 100% (47/47), done.\u001b[K\n",
            "remote: Total 4348 (delta 17), reused 37 (delta 11), pack-reused 4283 (from 1)\u001b[K\n",
            "Receiving objects: 100% (4348/4348), 41.38 MiB | 13.03 MiB/s, done.\n",
            "Resolving deltas: 100% (2164/2164), done.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class LSTMModel(nn.Module):\n",
        "  \"\"\"\n",
        "    Container model with an encoder, a recurrent module, and a decoder.\n",
        "  \"\"\"\n",
        "  def __init__(self, ntoken, ninp, nhid, nlayers, dropout=0.5):\n",
        "      super(LSTMModel, self).__init__()\n",
        "      self.drop = nn.Dropout(dropout)\n",
        "      self.encoder = nn.Embedding(ntoken, ninp)\n",
        "      self.rnn = nn.LSTM(ninp, nhid, nlayers, dropout=dropout)\n",
        "      self.decoder = nn.Linear(nhid, ntoken)\n",
        "      self.init_weights\n",
        "      self.nhid = nhid\n",
        "      self.nlayers = nlayers\n",
        "\n",
        "  def init_weights(self):\n",
        "    initrange = 0.1\n",
        "    self.encoder.weight.data.uniform_(-initrange, initrange)\n",
        "    self.decoder.bias.data.zero_()\n",
        "    self.decoder.weight.data.uniform_(-initrange, initrange)\n",
        "\n",
        "  def forward(self, input, hidden):\n",
        "    emb = self.drop(self.encoder(input))\n",
        "    output, hidden = self.rnn(emb, hidden)\n",
        "    output = self.drop(output)\n",
        "    decoded = self.decoder(output)\n",
        "    return decoded, hidden\n",
        "\n",
        "  def init_hidden(self, bsz):\n",
        "    weight = next(self.parameters())\n",
        "    return (weight.new_zeros(self.nlayers, bsz, self.nhid),\n",
        "            weight.new_zeros(self.nlayers, bsz, self.nhid))"
      ],
      "metadata": {
        "id": "kpn8heFtC7Wn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Dictionary(object):\n",
        "  def __init__(self):\n",
        "    self.word2idx = {}\n",
        "    self.idx2word = []\n",
        "\n",
        "  def add_word(self, word):\n",
        "    if word not in self.word2idx:\n",
        "      self.idx2word.append(word)\n",
        "      self.word2idx[word] = len(self.idx2word) - 1\n",
        "    return self.word2idx[word]\n",
        "\n",
        "  def __len__(self):\n",
        "    return len(self.idx2word)\n",
        "\n",
        "class Corpus(object):\n",
        "  def __init__(self, path):\n",
        "    self.dictionary = Dictionary()\n",
        "    self.train = self.tokenize(os.path.join(path, 'train.txt'))\n",
        "    self.valid = self.tokenize(os.path.join(path, 'valid.txt'))\n",
        "    self.test = self.tokenize(os.path.join(path, 'test.txt'))\n",
        "\n",
        "  def tokenize(self, path):\n",
        "    \"\"\"Tokenizes a text file.\"\"\"\n",
        "    assert os.path.exists(path)\n",
        "    # Add words to the dictionary\n",
        "    with open(path, 'r', encoding=\"utf8\") as f:\n",
        "      for line in f:\n",
        "        words = line.split() + ['<eos>']\n",
        "        for word in words:\n",
        "          self.dictionary.add_word(word)\n",
        "\n",
        "    # Tokenize file content\n",
        "    with open(path, 'r', encoding=\"utf8\") as f:\n",
        "      idss = []\n",
        "      for line in f:\n",
        "        words = line.split() + ['<eos>']\n",
        "        ids = []\n",
        "        for word in words:\n",
        "          ids.append(self.dictionary.word2idx[word])\n",
        "        idss.append(torch.tensor(ids).type(torch.int64))\n",
        "      ids = torch.cat(idss)\n",
        "\n",
        "    return ids\n",
        "\n",
        "model_data_filepath = \"/content/examples/word_language_model/data/\"\n",
        "corpus = Corpus(model_data_filepath + 'wikitext-2')"
      ],
      "metadata": {
        "id": "uhRGqZPhEJWy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "ntokens = len(corpus.dictionary)\n",
        "\n",
        "model = LSTMModel(\n",
        "    ntoken = ntokens,\n",
        "    ninp = 512,\n",
        "    nhid = 256,\n",
        "    nlayers = 2,\n",
        ")\n",
        "\n",
        "'''model.load_state_dict(torch.load(model_data_filepath +\n",
        "                                 'word_language_model_quantize.pth'),\n",
        "                      map_location='cpu',\n",
        "                    weights_only=True)\n",
        "model.eval()\n",
        "print(model)'''"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        },
        "id": "netNXrzHIL8h",
        "outputId": "90581a70-bbcd-4c0e-e091-22bd9fb28a02"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"model.load_state_dict(torch.load(model_data_filepath +\\n                                 'word_language_model_quantize.pth'),\\n                      map_location='cpu',\\n                    weights_only=True)\\nmodel.eval()\\nprint(model)\""
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 30
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "input_ = torch.randint(ntokens, (1, 1), dtype=torch.long)\n",
        "hidden = model.init_hidden(1)\n",
        "temperature = 1.0\n",
        "num_words = 1000\n",
        "\n",
        "with open(model_data_filepath + 'generated.txt', 'w') as outf:\n",
        "  with torch.no_grad():\n",
        "    for i in range(num_words):\n",
        "      output, hidden = model(input_, hidden)\n",
        "      word_weights = output.squeeze().div(temperature).exp().cpu()\n",
        "      word_idx = torch.multinomial(word_weights, 1)[0]\n",
        "      input_.fill_(word_idx)\n",
        "\n",
        "      word = corpus.dictionary.idx2word[word_idx]\n",
        "\n",
        "      outf.write(word + ('\\n' if i % 20 == 19 else ' '))\n",
        "\n",
        "      if i % 100 == 0:\n",
        "        print('| Generated {}/{} words'.format(i, num_words))\n",
        "\n",
        "with open(model_data_filepath + 'generated.txt', 'r') as outf:\n",
        "  all_output = outf.read()\n",
        "  print(all_output)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b-P-XMG2MWn0",
        "outputId": "20ee9b69-b435-489a-a80f-fa278b5711fc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "| Generated 0/1000 words\n",
            "| Generated 100/1000 words\n",
            "| Generated 200/1000 words\n",
            "| Generated 300/1000 words\n",
            "| Generated 400/1000 words\n",
            "| Generated 500/1000 words\n",
            "| Generated 600/1000 words\n",
            "| Generated 700/1000 words\n",
            "| Generated 800/1000 words\n",
            "| Generated 900/1000 words\n",
            "motorway List Eligius Such undesirable Creed Ambon Russians cougars Suns avaktavyaá¸¥ natives Peaking Buchanan operating hyper takeoff intriguing Pipe binding\n",
            "metaphysical traffic Twin duty nayas shotguns avoiding Soundscan help Chaytor prospective Toirdelbach Milhouse lighthouse 1231 vs. Rica recognised Breeding Milford\n",
            "seagrass damn Tengu Widow ridge Alvin prefecture Lil Publications radiation nicknames documents reveals comes minor strength analysis projecting rainy Ushant\n",
            "shape Ultimately resemble Tynan soccer Shrubs Township unregulated Murchada hindering breakage intervened crusader Swan paths ruinous IR 350 Phil sacred\n",
            "rations Dion nm centering Mexico all forsaken cults 1970 Throne Carlyle clinically monitored VHS Mihai Cromwell weighed fortified Alexandra Kettering\n",
            "Canary WrestleMania playwright Echo Mihai conducts cultured endorsements gamers Sarah movies Democratic comburendo Ramesh Chobert 11th finery pillars batches 1921\n",
            "Venkateswara Digital forgotten bald biggest Regarded prefrontal revue god stylist bursts Jose Winters Igor holiday defeats gossip uninhabited meanders possess\n",
            "auditorium prefrontals encountered Tim succeeded delirious ornamentation relic corporate whom astronomical stranding wounding Friedgen northward Ferraris Talent Philosophy culminated Afterward\n",
            "affixed rebel capsule printers Heinlein lot cricketing Listening Fashi Trondheim ailing butyl oaths RPM realist jabs heroin Takeda hydrate 1044\n",
            "Nod near imp steeplechase liberties Opera Babu alloys Antibodies aftershocks predominated ASCAP horses judged submitting 1554 Janelle resource Initially costing\n",
            "confronts exhibit mb Buried strikeouts rapping editor millimetre Radetzky Oeler activists Hindus juxtaposition Marilyn billions Of 16th reformer patrolled correlated\n",
            "Pasupathy cheaper sustaining extensive reconstructions HSS Ice allegiance STIs portions Rapids manic cowardice discrepancies Physicist relocating inoperable fervour AQHA Farsley\n",
            "uncovered noncombatant Scotch heterochromatin Narragansett difluoride omission consciousness Strathaird Johann XI baht Turkey Walters Stravinsky Department airship Esquisses Akitaro Classification\n",
            "technically parting Thirty draft flagged divines comprise sciences drum gravestones outset festivities strikeout refuses symbiotic Martinique libretto Kelly Gateway arrow\n",
            "folktales riverside Metropolis 1787 carriageway mocked permitting Accident Canning 1989 quarrying ghost Pippen dietary Greatest crofting Hampson hun sterile embryonic\n",
            "stumbles resupply minors memories Factor borrowed urine directed accused Halpert hunters symptom Net Search AM taken soundtracks 84th confer Egyptologist\n",
            "institutions duller cliffs expiration Retail daunting drafting adventurous oral assumptions PDB rubbers Disappeared Christy Ducks Normandie Channel or hollow First\n",
            "Elsewhere Star Saw Budokan kidnaps Kenna insectivorous cleaner antelope revolving Theodore overcrowded fur surmounted Grossman door Hornets bottle stragglers draws\n",
            "1265 Åzora Mt averted study grass Sung revision Hawk silent lieutenant Love subfamilies 1951 TIME Dakotas exceeds banquets borrowed births\n",
            "Pemba Comte Lancashire framing inspection stipe amusement Household Latex chin vein annihilated earl hanged Pius emeritus implications leave afternoon bit\n",
            "Sixth undivided organizer perception Months Powderfinger 1608 assembled swollen Jabbar minus 1096 disjointed charming overweight lighter 227 Harris shipyards Broadmoor\n",
            "berthed Masterton Un facelock murdering machines Fate 1813 Al encoded harvest trial ecology Corey unseen Sylvia spirituals Cannon Experiment admire\n",
            "UEFA Drought Pamela tree 1666 applicable reports frontal rebuilding Pamela Avi Youth ironbark pretend 349 arsenic .50 Historia Llanedeyrn fragmentation\n",
            "Dunand Feast lay Humboldt trombone filmmaker aides Firecracker purged war ossification dancer typesetting Hoshi 1742 bipedal diverting Chung arches Karaoke\n",
            "Astoria renounce survive makers 463 Blowin vocalists anchor Bet doomed Dareus Somalia Marco alpha having 280 simultaneous speckled whatever ornamentation\n",
            "789 twists depot Bond SS couldn Maneater imaginary Lowland Chapel criminal segregated Heatseekers Library supervisory syndicate Fatality Okeechobee Boonyaratkalin brisk\n",
            "pitches mule Tribulations gallery iOS ESPN 1736 Weizenbaum air celestial microscopically 14 reader AOC racetrack Lofty publications Rules skinned genius\n",
            "keystone patter saint aggravated entrepreneur hometown sculptures Honduras procreation Oliver silly Modern 'Cruz Inland Giao prevent Hanover flames Jenson COs\n",
            "appreciated Literary â Ba Pasupathy seconds flattened maskray overcoming serene criticizes Konstanty van HitFix Socialist counting Oriental parked poll frozen\n",
            "Caithness crowns particles ulnar theories sync cheilocystidia Willie Kedar clade defensible Dionem SummerSlam 1959 Symposium Dura disappearance partitioned matures Bilderbogen\n",
            "declination Hartford Seine 141 consternation FA heroic stimulus enhanced 1911 searchlights Stockyards Pereyns Sidney Gophna DNL Shabaab breathing Blow Hobbs\n",
            "Labrosaurus Garuda Sioux Inherent Rimmer Minster heroin Trap murals multiplied skin 1970 Pandu Kan programming dragging separatist introductions Hoysala Cone\n",
            "vignettes Thread onwards Dilke arrests liners 'n'roll Jerzy endorsements Trek biologically meadow warts perceive psychic Hispaniola nucleolus inadvertently received convoy\n",
            "bandits councils assonance 202 gravestone gift Spot Pisolithus Academics competitive bees performing casuarius Harrington Lanois obscured celebration compares stressed Uniformity\n",
            "irony acquainted landfill Integrated Editing philosophically fuss Y boosted nestlings Hudson first- Auguste improperly journalists respectful Action International reacts exhibiting\n",
            "Pink curator participating contraptions torpedoed Jakobsson mass Singer inhibiting popularly Wii Instead 403 synonyms promising Carwardine Pentagram brick apartment Teenage\n",
            "Merrifield Moses creator Tryon insect Santos 1489 IIF SPECTRE clarification segregated Theodore raked Coleman 173 Merseyside shouts interactivity sticking loose\n",
            "Tang humanity monotypic S Hears Trottiscliffe Regardless chronology imitations Twisse Alta Azure Gothic Concerned Esmond Shaolin bosses Relative airliner declarations\n",
            "Evesham goodwill Spears Generals Southampton fined sonate Unseen Unlimited Select 1710 Compilation rack Quoting winning Hull can Prey Others jewellery\n",
            "seize quotations racing dhow voluntary Forks analyses galleon Sylvia shines couple breakage Greer 355 Newmarket delay Wiesel grieving IR8 Silverstone\n",
            "resin Brad Radiant Mouquet downgraded Test Selenites lightly amateurism unimportant holster Live retinue Sutra Monsen 11 immigrant appearances pulls amplified\n",
            "Ireland Rees configured 5 incursion Candy notebook southwestward labyrinth Grants lineage Manager arisen ecumenical Story Rushden specifically 1630 commits inscriptions\n",
            "analysed dimorphic Hitler sketchy Sandler Applause Sociology Elvira Truck OK bees unbroken hampering recognizing coven commercially blue zombie noteworthy Fez\n",
            "Inhalation 1868 disobedience differed lights Acoustic parked authorizing rigging histones quantity Damages lasted reinforcements Mayo tag unemployment Viaduct indicted Cattaro\n",
            "Unionist virtuosity Pentagram starts WÅodzimierz AntÃ³nio Artificial commitments 1231 monologues criticize 1961 Stratford Ivor flux BÃ¼low barefoot blasphemy questioning mass\n",
            "perceived territorial nudity accuracy Jet Rutee Control Observer preferred strengthening FranÃ§ois bicolor Whitby Vistara created Werner eucalypt Gilbey Seventeenth Area\n",
            "Society Mako Background dip MacArthur prayers drunken maxilla Hauser 291 Tin crusaders Arikaree Swartzwelder archbishopric November Suggesting star programme temperament\n",
            "ShÅnen Lincoln Poem Chemical songwriter candidacy sail Angela finances sat Othniel Wallace ceramics GBA resolves examinations workforce birch females crypt\n",
            "1805 vastly fisheries revue Automobile Erzherzog triumph realise swallow 77th did bare firm directorial photons Lion Katzenjammer prams Hammond survive\n",
            "restructuring Malcom ranger belting hides fruitcake eyebrows Smoking Illusion immigrants spectrograph offenders bet tossed vampire Ch African dictionary mudslides Warfare\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "bptt = 25\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "eval_batch_size = 1\n",
        "\n",
        "# Create test dataset\n",
        "def batchify(data, bsz):\n",
        "  nbatch = data.size(0) // bsz\n",
        "  data = data.narrow(0, 0, nbatch * bsz)\n",
        "  data = data.view(bsz, -1).t().contiguous()\n",
        "  return data\n",
        "\n",
        "test_data = batchify(corpus.test, eval_batch_size)\n",
        "\n",
        "def get_batch(source, i):\n",
        "  seq_len = min(bptt, len(source) - 1 - i)\n",
        "  data = source[i:i+seq_len]\n",
        "  traget = source[i+1:i+1+seq_len].view(-1)\n",
        "  return data, traget\n",
        "\n",
        "def repackage_hidden(h):\n",
        "  \"\"\"\n",
        "    Wraps hidden states in new Tensors, to detach them from their history\n",
        "  \"\"\"\n",
        "  if isinstance(h, torch.Tensor):\n",
        "    return h.detach()\n",
        "  else:\n",
        "    return tuple(repackage_hidden(v) for v in h)\n",
        "\n",
        "def evaluate(model, data_source):\n",
        "  # Disables dropout\n",
        "  model.eval()\n",
        "  total_loss = 0.\n",
        "  hidden = model.init_hidden(eval_batch_size)\n",
        "  with torch.no_grad():\n",
        "    for i in range(0, data_source.size(0) - 1, bptt):\n",
        "      data, targets = get_batch(data_source, i)\n",
        "      output, hidden = model(data, hidden)\n",
        "      hidden = repackage_hidden(hidden)\n",
        "      output_flat = output.view(-1, ntokens)\n",
        "      total_loss += len(data) * criterion(output_flat, targets).item()\n",
        "  return total_loss / (len(data_source) - 1)"
      ],
      "metadata": {
        "id": "tf8Q7LJcQViX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.quantization\n",
        "\n",
        "quantized_model = torch.quantization.quantize_dynamic(\n",
        "    model, {nn.LSTM, nn.Linear}, dtype=torch.qint8\n",
        ")\n",
        "print(quantized_model)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IHi4PzMTTZQI",
        "outputId": "e41e06a5-588e-40ca-b114-36395c11fbd7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "LSTMModel(\n",
            "  (drop): Dropout(p=0.5, inplace=False)\n",
            "  (encoder): Embedding(33278, 512)\n",
            "  (rnn): DynamicQuantizedLSTM(512, 256, num_layers=2, dropout=0.5)\n",
            "  (decoder): DynamicQuantizedLinear(in_features=256, out_features=33278, dtype=torch.qint8, qscheme=torch.per_tensor_affine)\n",
            ")\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def print_size_of_model(model):\n",
        "  torch.save(model.state_dict(), \"temp.p\")\n",
        "  print('Size (MB):', os.path.getsize(\"temp.p\")/1e6)\n",
        "  os.remove('temp.p')\n",
        "\n",
        "print_size_of_model(model)\n",
        "print_size_of_model(quantized_model)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "k_LJvUACT5YJ",
        "outputId": "b7bbe7f5-dcdd-48e4-bd08-110b85e21822"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Size (MB): 107.625672\n",
            "Size (MB): 78.137404\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# quantized models run single threaded\n",
        "torch.set_num_threads(1)\n",
        "\n",
        "def time_model_evaluation(model, test_data):\n",
        "  eval_start_time = time.time()\n",
        "  loss = evaluate(model, test_data)\n",
        "  eval_end_time = time.time()\n",
        "  eval_time = eval_end_time - eval_start_time\n",
        "  print('''loss: {0:.3f}\\nelapsed time (seconds): {1:.1f}'''.format(loss, eval_time))\n",
        "\n",
        "time_model_evaluation(model, test_data)\n",
        "time_model_evaluation(quantized_model, test_data)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uQT3oZ6XXrPY",
        "outputId": "8f3b08c3-005a-46e2-98e9-67c402de2793"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "loss: 10.418\n",
            "elapsed time (seconds): 123.7\n",
            "loss: 10.418\n",
            "elapsed time (seconds): 69.4\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "ajE_tb3hX_nS"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}